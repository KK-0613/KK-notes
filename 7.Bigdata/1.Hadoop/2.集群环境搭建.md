## 1.内容回顾

|-- 创建虚拟机，机器名102
  |-- jdk的压缩包 /opt/software目录下
  |-- jdk的解压后 /opt/module目录下
  |-- 配置JAVA_HOME的环境变量

```properties
|-- /etc/profile 文件，是Linux系统的环境变量配置文件
|-- /etc/profile.d目录下，创建自己的shell脚本文件 my_evn.sh
  |-- JAVA_HOME配置在 my_evn.sh
  |-- /etc/profile 文件，自动读取profile.d目录下的shell脚本文件
|-- 生效环境变量：source /etc/profile
```

  |-- hadoop的压缩包 /opt/software目录下 
  |-- hadoop的解压后 /opt/module目录下
  |-- 配置HADOOP_HOME环境变量
​    |-- 配置方式和JDK一样
​    |-- hadoop环境变量要配置两个目录,bin,sbin

  |-- hadoop组成部分
​    |-- hdfs -- hadoop 分布式文件系统
​      |-- namenode服务：存储文件的元数据信息
​        |-- 文件名，大小，创建时间，块信息，hdfs中的存储位置

      |-- datanode服务：存储文件的，文件切割，最大存储块128MB
        |-- 多机器存储，搭建集群环境，一个文件在所有的机器都存储
    
      |-- secondarynamenode服务:备份元数据的服务

|-- **yarn -- 资源调度平台**
  |-- yarn 就是Java写好的mapreduce程序的运行平台
  |-- 负责分配内存，分配CPU
  |-- resourcemanager服务，所有yarn服务的总头领
    |-- nodemanager 真正的程序运行平台

|-- mapreduce -- java写的程序

## 2. 集群资料准备：scp 文件分发

- 需求：scp命令，将102机器的内容发送到103,104
  - 102机器：/opt/module 解压后的内容 jdk hadoop
  - 发到103和104，目录一致性

```properties
scp命令发送：scp -r 要发送的内容 另一台机器的用户名@主机名:要发送到的位置
```

- scp命令，将102机器的jdk（解压缩后的）发到103机器 （推送）

```properties
scp -r jdk1.8.0_212 atguigu@hadoop103:/opt/module/
```

- scp命令，在103机器，从102机器上拉取hadoop的解压后 （拉取）
  - 发送之间，建议删除102机器，hadoop解压目录/share/doc目录

```properties
        数据源机器用户名@机器名 数据源路径名             拉取到的位置
scp -r atguigu@hadoop102:/opt/module/hadoop-3.1.3 /opt/module/
```

- scp命令，在103机器，从102机器上拉取jdk，推送到104机器

```properties
        数据源机器:数据源路径                  目标机器：目标机路径
scp -r hadoop102:/opt/module/jdk1.8.0_212 hadoop104:/opt/module/
scp -r hadoop102:/opt/module/hadoop-3.1.3 hadoop104:/opt/module/
#如果不写登录其他机器的用户名，默认使用当前机器的登录名，当前103机器登录的用户名
```

> 注意：执行scp命令的时候，无论是数据源机器，还是目标机器，都不要操作，等文件传递完毕在操作

- rsync命令：机器之间的文件传递，做差异化传输，已经存在的文件，不在复制了

```
rsync命令：将102机器上的/opt/module所有内容，传递到103机器，差异化复制
```

## 3.同步环境变量

将102机器上：/etc/profile.d目录下的文件my_env.sh，同步到103和104机器上。

> 注意问题：同步103和104保证 my_env.sh 存储位置必须是/etc/profile.d

```properties
#执行失败 scp: /etc/profile.d/my_env.sh: Permission denied 权限不足
scp -r my_env.sh hadoop103:/etc/profile.d
同步103机器，登录103机器是root账户
scp -r my_env.sh root@hadoop103:/etc/profile.d
scp -r my_env.sh root@hadoop104:/etc/profile.d
目标机器加载环境变量配置文件：source /etc/profile
```

![](https://iamgs.oss-cn-shanghai.aliyuncs.com/Images/05281.png)

## 4. 服务器集群分发文件，shell脚本

scp命令可以实现机器之间的文件分发，机器数量大，造成有些机器没有发，有些机器发了，人的工作量就机器强大，因此定义shell脚本代码，同一发送，集群分发。

shell脚本：目的是在集群中做文件分发，102机器为数据源机器，发送/opt目录下的内容，也可能会发送/etc/profile.d目录的内容。

> 要求：群发的shell脚本文件可以在任意路径下执行，shell脚本文件放在哪里就很关键，shell代码放在自己的账户目录的bin下面

![](https://iamgs.oss-cn-shanghai.aliyuncs.com/Images/05282.png)

- xsync.sh

```shell
#!/bin/bash
#1. 判断参数个数
if [ $# -lt 1 ]
then
  echo Not Enough Arguement!
  exit;
fi
#2. 遍历集群所有机器
for host in hadoop103 hadoop104
do
  echo ====================  $host  ====================
  #3. 遍历所有目录，挨个发送
  for file in $@
  do
    #4. 判断文件是否存在
    if [ -e $file ]
    then
      #5. 获取父目录
      pdir=$(cd -P $(dirname $file); pwd)
      #6. 获取当前文件的名称
      fname=$(basename $file)
      ssh $host "mkdir -p $pdir"
      rsync -av $pdir/$fname $host:$pdir
    else
      echo $file does not exists!
    fi
  done
done

```

## 5.SSH安全外壳协议

SSH安全外壳协议目的免密登录：102,103,104 机器之间无论是谁连接谁都不在需要密码确认。

![](https://iamgs.oss-cn-shanghai.aliyuncs.com/Images/05283.png)

- 在102机器，产生ssh协议的密钥对（公钥，私有）

```properties
ssh-keygen -t rsa
密钥对产生位置：账户目录
id_rsa 私钥
id_rsa.pub 公钥
```

- 102机器，将公钥授权给103机器，104机器

```properties
ssh-copy-id hadoop103
ssh-copy-id hadoop104
ssh-copy-id hadoop102 给自己也授权
```

- 在103机器，产生ssh协议密钥对

```properties
ssh-keygen -t rsa
密钥对产生位置：账户目录
id_rsa 私钥
id_rsa.pub 公钥
```

- 在103机器，将公钥授权给102机器，104机器

```properties
ssh-copy-id hadoop103
ssh-copy-id hadoop104
ssh-copy-id hadoop102 给自己也授权
```

- 在104机器，产生ssh协议的密钥对（公钥，私有）
- 在104机器，将公钥授权给102机器，103机器

## 6. 集群配置文件分发

每台Hadoop服务器任务：节约内存，保证数据安全

|      |      hadoop102      |           hadoop103           |          hadoop104           |
| ---- | :-----------------: | :---------------------------: | :--------------------------: |
| HDFS | NameNode   DataNode |           DataNode            | SecondaryNameNode   DataNode |
| YARN |     NodeManager     | ResourceManager   NodeManager |         NodeManager          |

- 配置文件：在102机器中配置，分发到103,104机器

- Hadoop的用户配置文件：/opt/module/hadoop-3.1.3/etc/hadoop

  - core-site.xml 核心配置文件

  ```xml
  <configuration>
  	<!-- 指定NameNode的地址 -->
      <property>
          <name>fs.defaultFS</name>
          <value>hdfs://hadoop102:9820</value>
      </property>
  <!-- 指定hadoop数据的存储目录 -->
      <property>
          <name>hadoop.tmp.dir</name>
          <value>/opt/module/hadoop-3.1.3/data</value>
  </property>
  
  <!-- 配置HDFS网页登录使用的静态用户为atguigu -->
      <property>
          <name>hadoop.http.staticuser.user</name>
          <value>atguigu</value>
  </property>
  
  <!-- 配置该atguigu(superUser)允许通过代理访问的主机节点 -->
      <property>
          <name>hadoop.proxyuser.atguigu.hosts</name>
          <value>*</value>
  </property>
  <!-- 配置该atguigu(superUser)允许通过代理用户所属组 -->
      <property>
          <name>hadoop.proxyuser.atguigu.groups</name>
          <value>*</value>
  </property>
  <!-- 配置该atguigu(superUser)允许通过代理的用户-->
      <property>
          <name>hadoop.proxyuser.atguigu.users</name>
          <value>*</value>
  </property>
  
  </configuration>
  
  ```

  - hdfs-site.xml

  ```xml
  <configuration>
  	<!-- nn web端访问地址-->
  	<property>
          <name>dfs.namenode.http-address</name>
          <value>hadoop102:9870</value>
      </property>
  	<!-- 2nn web端访问地址-->
      <property>
          <name>dfs.namenode.secondary.http-address</name>
          <value>hadoop104:9868</value>
      </property>
  </configuration>
  
  ```

  - yarn-site.xml

  ```xml
  <?xml version="1.0" encoding="UTF-8"?>
  <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
  
  <configuration>
  	<!-- 指定MR走shuffle -->
      <property>
          <name>yarn.nodemanager.aux-services</name>
          <value>mapreduce_shuffle</value>
  </property>
  <!-- 指定ResourceManager的地址-->
      <property>
          <name>yarn.resourcemanager.hostname</name>
          <value>hadoop103</value>
  </property>
  <!-- 环境变量的继承 -->
      <property>
          <name>yarn.nodemanager.env-whitelist</name>
          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
  </property>
  <!-- yarn容器允许分配的最大最小内存 -->
      <property>
          <name>yarn.scheduler.minimum-allocation-mb</name>
          <value>512</value>
      </property>
      <property>
          <name>yarn.scheduler.maximum-allocation-mb</name>
          <value>4096</value>
  </property>
  <!-- yarn容器允许管理的物理内存大小 -->
      <property>
          <name>yarn.nodemanager.resource.memory-mb</name>
          <value>4096</value>
  </property>
  <!-- 关闭yarn对物理内存和虚拟内存的限制检查 -->
      <property>
          <name>yarn.nodemanager.pmem-check-enabled</name>
          <value>false</value>
      </property>
      <property>
          <name>yarn.nodemanager.vmem-check-enabled</name>
          <value>false</value>
      </property>
  </configuration>
  
  ```

  - mapred-site.xml

  ```xml
  <configuration>
  	<!-- 指定MapReduce程序运行在Yarn上 -->
      <property>
          <name>mapreduce.framework.name</name>
          <value>yarn</value>
      </property>
  </configuration>
  
  ```

- 从102机器发送配置文件

  - 群发脚本

  `xsync.sh hdfs-site.xml core-site.xml yarn-site.xml mapred-site.xml`

## 7. 手动开启服务（了解）

- 102机器上，对hadoop格式化

```properties
hdfs namenode -format
在Hadoop的解压目录下，bin下执行
```

-  hadoop102  : 开启元数据服务namenode，和存储数据服务datanode
   - `hdfs --daemon start namenode`
   - `hdfs --daemon start datanode`

-  hadoop103：开启yarn服务resourceManager，nodenanager,datanode
   - `yarn --daemon start resourcemanager`
   - `yarn --daemon start nodemanager`
   - `hdfs --daemon start datanode`
   - 返回102机器，启动yarn平台`yarn --daemon start nodemanager`
-  hadoop104：开启secondrayname，nodenanager,datanode
   - `hdfs --daemon start secondarynamenode`
   - `hdfs --daemon start datanode`
   - `yarn --daemon start nodemanager`

## 8. 自动服务（群起）

- 服务器群起命令shell代码：`/opt/module/hadoop-3.1.3/sbin`

- 服务器群起修改配置文件：`/opt/module/hadoop-3.1.3/etc/hadoop/workers` ：分发103,104

  ```properties
  hadoop102
  hadoop103
  hadoop104
  ```

- 群起 hdfs：102机器

  - `start-dfs.sh`

- 群起yarn: 103机器

  - `start-yarn.sh`

- 群起群停shell脚本

  - 放在自己的账户目录/bin下（my_hadoop）

  ```shell
  #!/bin/bash
  if [ $# -lt 1 ]
  then
      echo "No Args Input..."
      exit ;
  fi
  case $1 in
  "start")
          echo " =================== 启动 hadoop集群 ==================="
  
          echo " --------------- 启动 hdfs ---------------"
          ssh hadoop102 "/opt/module/hadoop-3.1.3/sbin/start-dfs.sh"
          echo " --------------- 启动 yarn ---------------"
          ssh hadoop103 "/opt/module/hadoop-3.1.3/sbin/start-yarn.sh"
          echo " --------------- 启动 historyserver ---------------"
          ssh hadoop102 "/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver"
  ;;
  "stop")
          echo " =================== 关闭 hadoop集群 ==================="
  
          echo " --------------- 关闭 historyserver ---------------"
          ssh hadoop102 "/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver"
          echo " --------------- 关闭 yarn ---------------"
          ssh hadoop103 "/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh"
          echo " --------------- 关闭 hdfs ---------------"
          ssh hadoop102 "/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh"
  ;;
  *)
      echo "Input Args Error..."
  ;;
  esac
  
  ```

  ## 9. 集群功能测试

`hadoop jar hadoop-mapreduce-examples-3.1.3.jar wordcount /wcinput /wcoutput`











# 框架运行出错怎么办

1. 定位错误的进程（我们的例子是Datanode）

2. 查看错误进程的日志（去DN挂掉的节点，找到其日志，例子里面我们查看hadoop102的DN日志）

3. 定位运行日志位置

   日志的位置就在Hadoop家目录下logs文件夹里面

4. 查看日志

   ```bash
   tail -n 100 /opt/module/hadoop-3.1.3/logs/hadoop-atguigu-datanode-hadoop102.log
   ```

   ```
   2022-05-30 08:46:50,265 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@3406472c{HTTP/1.1,[http/1.1]}{localhost:33538}
   2022-05-30 08:46:50,265 INFO org.eclipse.jetty.server.Server: Started @1003ms
   2022-05-30 08:46:50,338 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864
   2022-05-30 08:46:50,341 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
   2022-05-30 08:46:50,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = atguigu
   2022-05-30 08:46:50,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
   2022-05-30 08:46:50,392 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
   2022-05-30 08:46:50,402 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9867
   2022-05-30 08:46:50,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:9867
   2022-05-30 08:46:50,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
   2022-05-30 08:46:50,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
   2022-05-30 08:46:50,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop102/192.168.10.102:8020 starting to offer service
   2022-05-30 08:46:50,525 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
   2022-05-30 08:46:50,525 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9867: starting
   2022-05-30 08:46:50,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop102/192.168.10.102:8020
   2022-05-30 08:46:50,678 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
   2022-05-30 08:46:50,681 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /opt/module/hadoop-3.1.3/data/dfs/data/in_use.lock acquired by nodename 2777@hadoop102
   2022-05-30 08:46:50,682 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/opt/module/hadoop-3.1.3/data/dfs/data
   java.io.IOException: Incompatible clusterIDs in /opt/module/hadoop-3.1.3/data/dfs/data: namenode clusterID = CID-6521df2b-56c9-4043-929d-01e57a85738e; datanode clusterID = CID-963fc533-014b-407e-a3cc-29021825ed8b
   	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:744)
   	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:294)
   	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:407)
   	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:387)
   	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:559)
   	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1743)
   	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1679)
   	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
   	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
   	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:822)
   	at java.lang.Thread.run(Thread.java:748)
   2022-05-30 08:46:50,683 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /opt/module/hadoop-3.1.3/data/dfs/data2/in_use.lock acquired by nodename 2777@hadoop102
   2022-05-30 08:46:50,683 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/opt/module/hadoop-3.1.3/data/dfs/data2
   java.io.IOException: Incompatible clusterIDs in /opt/module/hadoop-3.1.3/data/dfs/data2: namenode clusterID = CID-6521df2b-56c9-4043-929d-01e57a85738e; datanode clusterID = CID-963fc533-014b-407e-a3cc-29021825ed8b
   	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:744)
   	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:294)
   	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:407)
   	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:387)
   	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:559)
   	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1743)
   	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1679)
   	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
   	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
   	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:822)
   	at java.lang.Thread.run(Thread.java:748)
   2022-05-30 08:46:50,684 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid c2c33d95-c940-471a-89e0-e6b734140b06) service to hadoop102/192.168.10.102:8020. Exiting. 
   java.io.IOException: All specified directories have failed to load.
   	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:560)
   	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1743)
   	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1679)
   	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
   	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:282)
   	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:822)
   	at java.lang.Thread.run(Thread.java:748)
   2022-05-30 08:46:50,684 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid c2c33d95-c940-471a-89e0-e6b734140b06) service to hadoop102/192.168.10.102:8020
   2022-05-30 08:46:50,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid c2c33d95-c940-471a-89e0-e6b734140b06)
   2022-05-30 08:46:52,785 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
   2022-05-30 08:46:52,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
   /************************************************************
   SHUTDOWN_MSG: Shutting down DataNode at hadoop102/192.168.10.102
   ************************************************************/
   ```

5. 定位日志错误

   日志按行分为四个级别：

   | 级别  | 意义                                                     |
   | ----- | -------------------------------------------------------- |
   | INFO  | 框架正常运行的日志，一般不用管                           |
   | WARN  | 警告：需要提起注意的地方，如果其没有导致错误，可以不用管 |
   | ERROR | 错误：框架运行不正常，需要修复                           |
   | FATAL | 致命错误：框架因为这个挂掉了                             |

   如果需要排错，找ERROR和FATAL

   在我们的例子里面：错误是因为NN和DN的集群ID不一致导致

6. 修复错误

   将NN的VERSION文件的clusterID改为和DN一致，重启集群



# 上课过程中常用的脚本

1. xsync 集群文件同步脚本

```sql
xsync /opt/module/hadoop-3.1.3/etc/hadoop/
```

1. xcall 集群执行指令【集群同步执行同一个命令】

   ```sql
   # 先安装pdsh
   sudo yum install -y epel-release
   sudo yum install -y pdsh
   ```

   ```bash
   # pdsh使用帮助
   pdsh -w 'hadoop102,hadoop103,hadoop104' 'jps'
   ```

   ```bash
   # 封装xcall脚本
   sudo vim /bin/xcall
   ```

   粘贴如下内容：

   ```bash
   #!/bin/bash
   pdsh -w 'hadoop102,hadoop103,hadoop104' "$*" | awk -F ":" '{temp=$1;$1=null;array[temp]=array[temp]$0"\n"}END{for (i in array) {print "===========  "i"  ============\n"array[i]}}'
   ```

   ```bash
   # 保存退出，增加执行权限
   sudo chmod +x /bin/xcall
   ```

2. jpsall 快速查看三个节点的Java进程

   ```bash
   # 封装jpsall脚本
   sudo vim /bin/jpsall
   ```

   粘贴如下内容：

   ```bash
   #!/bin/bash
   xcall jps | grep -v Jps
   ```

   ```bash
   # 保存退出，增加执行权限
   sudo chmod +x /bin/jpsall
   ```

   # 历史服务器和日志聚集

   1. mapred-site.xml

      添加如下内容

      ```xml
        <!-- 历史服务器端地址 -->
        <property>
          <name>mapreduce.jobhistory.address</name>
          <value>hadoop102:10020</value>
        </property>
      
        <!-- 历史服务器web端地址 -->
        <property>
          <name>mapreduce.jobhistory.webapp.address</name>
          <value>hadoop102:19888</value>
        </property>
      ```

   2. yarn-site.xml

      ```xml
        <!-- 开启日志聚集功能 -->
        <property>
          <name>yarn.log-aggregation-enable</name>
          <value>true</value>
        </property>
        <!-- 设置日志聚集服务器地址 -->
        <property>  
          <name>yarn.log.server.url</name>  
          <value>http://hadoop102:19888/jobhistory/logs</value>
        </property>
        <!-- 设置日志保留时间为7天 -->
        <property>
          <name>yarn.log-aggregation.retain-seconds</name>
          <value>604800</value>
        </property>
      ```

   3. 同步配置文件

      ```bash
      xsync  /opt/module/hadoop-3.1.3/etc
      ```

   4. 重启集群，并启动历史服务器

      ```bash
      # 关闭集群
      stop-dfs.sh
      stop-yarn.sh
      
      # 启动集群
      start-dfs.sh
      start-yarn.sh
      ```

      ```bash
      # 手动启动历史服务器
      mapred --daemon start historyserver
      ```

   5. 再跑一个mapreduce，测试历史服务器工作是否正常

      ```bash
      hadoop fs -mkdir /input
      hadoop fs -put /opt/module/hadoop-3.1.3/LICENSE.txt /input
      ```

      ```bash
      # 提交新的mr程序
      hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output
      ```

      完成以后，在网页上查看刚刚跑过的mr程序的日志信息



# 集群时间同步

1. NTP服务

   Network Time Protocol, 网络时间协议，是一个让本地计算机和网络上一台服务器时间保持一致的服务

   ```bash
   # 首先需要安装ntp服务
   sudo yum install -y ntp
   
   # 安装完成启动ntp服务
   sudo systemctl enable ntpd
   sudo systemctl start ntpd
   ```

2. 手动和网络上的服务器进行时间对齐

   ```bash
   sudo ntpdate ntp.aliyun.com
   ```

   ```bash
   # 如果给三台服务器对时间
   sudo xcall 'ntpdate ntp.aliyun.com'
   ```

3. 如果没有外部网络

   - 将102配置成能够提供时间同步服务的服务器

     ```bash
     # 首先需要安装ntp服务
     sudo yum install -y ntp
     
     # 安装完成启动ntp服务
     sudo systemctl enable ntpd
     sudo systemctl start ntpd
     ```

     ```bash
     # 配置102的ntp服务
     sudo vim /etc/ntp.conf
     ```

     修改如下位置

     将

     ```ini
     #restrict 192.168.10.0 mask 255.255.255.0 nomodify notrap
     ```

     改为

     ```ini
     restrict 192.168.10.0 mask 255.255.255.0 nomodify notrap
     ```

     将

     ```ini
     server 0.centos.pool.ntp.org iburst
     server 1.centos.pool.ntp.org iburst
     server 2.centos.pool.ntp.org iburst
     server 3.centos.pool.ntp.org iburst
     ```

     改为

     ```ini
     #server 0.centos.pool.ntp.org iburst
     #server 1.centos.pool.ntp.org iburst
     #server 2.centos.pool.ntp.org iburst
     #server 3.centos.pool.ntp.org iburst
     
     ```

     在任意位置添加

     ```ini
     server 127.127.1.0
     fudge 127.127.1.0 stratum 10
     
     ```

     保存退出，重启102的时间同步服务

     ```bash
     sudo systemctl restart ntpd
     
     ```

   - 103，104定期同步102时间

     ```bash
     # 在103和104设置定时任务
     sudo crontab -e
     
     ```

     添加

     ```bash
     * * * * * /usr/sbin/ntpdate hadoop102
     
     ```

     保存并退出

   - 改变103，104时间，看看能否自动纠正

     ```
     # 修改103，104时间
     sudo pdsh -w 'hadoop103,hadoop104' 'date -s "2011-01-01"'
     
     ```






   # 节点状态诊断

   1. 联网状态

      - 外网

        ```bash
        ping -c 4 www.baidu.com
        ```

        如果出现类似下面的显示，就没问题

        ```
        PING www.a.shifen.com (110.242.68.3) 56(84) bytes of data.
        64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=1 ttl=128 time=10.2 ms
        64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=2 ttl=128 time=10.4 ms
        64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=3 ttl=128 time=10.3 ms
        64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=4 ttl=128 time=10.5 ms
        
        --- www.a.shifen.com ping statistics ---
        4 packets transmitted, 4 received, 0% packet loss, time 3005ms
        rtt min/avg/max/mdev = 10.262/10.401/10.542/0.144 ms
        ```

      - 内网

        ```bash
        ping -c 4 hadoop102
        ping -c 4 hadoop103
        ping -c 4 hadoop104
        ```

   2. 权限状态

      - atguigu的sudo权限

        ```bash
        sudo ls
        ```

        如果不会报错就没事

      - /opt/module文件夹的所有者和所属组

        ```
        ll /opt/module
        ```

        查看所有者和所属组

        ```
        总用量 4
        drwxrwxr-x.  4 atguigu atguigu 4096 5月  30 09:36 datas
        drwxr-xr-x. 12 atguigu atguigu  190 5月  30 08:45 hadoop-3.1.3
        drwxrwxr-x. 10 atguigu atguigu  184 5月  21 11:16 hive
        drwxr-xr-x.  7 atguigu atguigu  245 4月   2 2019 jdk1.8.0_212
        drwxrwxr-x.  8 atguigu atguigu  160 5月  18 14:22 zookeeper
        ```

        如果不对，用以下指令修复

        ```bash
        sudo chown -R atguigu:atguigu /opt/*
        ```

   3. 免密配置

      - 免密设置

      ```properties
      ssh-keygen -t rsa
      密钥对产生位置：账户目录
      id_rsa 私钥
      id_rsa.pub 公钥
      ssh-copy-id hadoop103
      ```

      

      - atguigu免密

        ```
        ssh hadoop102
        ```

~~~properties
  ```
    sudo ssh hadoop104
    exit
    ```
  
    如果登录不上，用root用户互相配置一下免密
~~~

###    集群启动步骤

- 1）如果集群是第一次启动，需要在hadoop102节点格式化NameNode
  - 在hadoop的解压目录bin下

```
 hdfs namenode -format
```

- 群起hadoop服务

- web端查看HDFS的NameNode

  - ①浏览器中输入：http://hadoop102:9870

  - ②查看HDFS上存储的数据信息

- （5）Web端查看YARN的ResourceManager

  - ①浏览器中输入：http://hadoop103:8088

  - ②查看YARN上运行的Job信息

- 历史服务器的地址

  - http://hadoop102:19888/jobhistory
